{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZMBneLtMfFx"
      },
      "source": [
        "# Data Cleaning\n",
        "## Author: Amanda Belden\n",
        "### Master's Thesis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA2GnzQFO8ze"
      },
      "source": [
        "### Install Packages and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVpApUZVMkiX",
        "outputId": "8eeef6be-6fc6-428c-b0b5-98e946256960"
      },
      "outputs": [],
      "source": [
        "#pip install pgmpy\n",
        "#pip install feature_engine\n",
        "#pip install graphviz\n",
        "#pip install pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xXlEeqhDzwt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean NY ISRID Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkBrOGuIO5QR"
      },
      "source": [
        "### Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "_PreqaNmODSd",
        "outputId": "507e558c-dd74-41fa-db92-e3d1e9f3f7bd"
      },
      "outputs": [],
      "source": [
        "df_NYIncidents = pd.read_excel(\"NY_IncidentDataRequest.xlsx\")\n",
        "df_NYIncidents.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "di_NdJWVOz4I",
        "outputId": "b33464a9-3d2d-443b-ace4-d2518b72f207"
      },
      "outputs": [],
      "source": [
        "df_NYSubjects = pd.read_excel(\"NY_IncidentSubjectDataRequest.xlsx\")\n",
        "df_NYSubjects.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQpgDZjOPOF_"
      },
      "source": [
        "### Merge Datasets and Add Mission Hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyyuyG2ZeMd8"
      },
      "source": [
        "The code in this section was written by Julian Duran on the Probabilistic Reasoning AI4S&R Fall 2023 team."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge NY Subjects and NY Incidents Datasets from ISRID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jknHWRS0dYyg"
      },
      "outputs": [],
      "source": [
        "df_NYSubjects.drop([\"INCIDENT NUMBER\", \"INCIDENT ID NUMBER\", \"SUBJECT ID NUMBER\", \"EQUIPMENT USED\", \"TECHNIQUE\", \"FOUND IN SEARCH AREA\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "v_230BLwPQ5b",
        "outputId": "0c25f4f9-e346-4745-fcc1-b81e37b063e7"
      },
      "outputs": [],
      "source": [
        "df_ny_merged = pd.concat([df_NYIncidents, df_NYSubjects], axis=1)\n",
        "# df_ny_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkoTXcsGa5aB",
        "outputId": "7d6ed3a6-7192-4845-b604-24eadec439aa"
      },
      "outputs": [],
      "source": [
        "# df_ny_merged.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create total incident time and notified time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ODFaNdAbFAA"
      },
      "outputs": [],
      "source": [
        "df_ny_merged['TOTAL HOURS ISD'] = (pd.to_datetime(df_ny_merged[\"INCIDENT CLOSED DATE/TIME\"], format='%m/%d/%Y %I:%M %p') -\n",
        "                                   pd.to_datetime(df_ny_merged[\"INCIDENT START DATE\"], format='%m/%d/%Y %I:%M %p')).dt.total_seconds() / 3600\n",
        "\n",
        "df_ny_merged['TOTAL HOURS DON'] = (pd.to_datetime(df_ny_merged[\"INCIDENT CLOSED DATE/TIME\"], format='%m/%d/%Y %I:%M %p') -\n",
        "                                   pd.to_datetime(df_ny_merged[\"DUTY OFFIC NOTIFDATE\"], format='%m/%d/%Y %I:%M %p')).dt.total_seconds() / 3600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwQtWfsYeBcd"
      },
      "outputs": [],
      "source": [
        "df_ny_merged[\"TOTAL HOURS ISD\"] = df_ny_merged[\"TOTAL HOURS ISD\"].astype(int)\n",
        "df_ny_merged[\"TOTAL HOURS DON\"] = df_ny_merged[\"TOTAL HOURS DON\"].astype(int)\n",
        "\n",
        "df_ny_merged.loc[df_ny_merged[\"TOTAL HOURS ISD\"] < 0, \"TOTAL HOURS ISD\"] = np.nan\n",
        "df_ny_merged.loc[df_ny_merged[\"TOTAL HOURS DON\"] < 0, \"TOTAL HOURS DON\"] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFMFRcoVwezo"
      },
      "source": [
        "### Drop Unnecessary Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "3cra1NNAoEpi",
        "outputId": "2316a325-6042-409a-c004-944a939618c8"
      },
      "outputs": [],
      "source": [
        "# df_ny_merged.head()\n",
        "# df_ny_merged.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRZKdlnUwjyw"
      },
      "outputs": [],
      "source": [
        "df_ny = df_ny_merged.drop(columns=[\"INCIDENT ID NUMBER\", \"SUBJECT ID NUMBER\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean Dates into Year, Month, Day, Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create separate year, month, day of week, and time columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ny['INCIDENT START DATE'] = pd.to_datetime(df_ny['INCIDENT START DATE'])\n",
        "\n",
        "# Add a column for year \n",
        "df_ny['start_year'] = df_ny['INCIDENT START DATE'].dt.strftime('%Y')\n",
        "\n",
        "# Add a column for month name\n",
        "df_ny['start_month'] = df_ny['INCIDENT START DATE'].dt.strftime('%B')\n",
        "\n",
        "# Add a column for day of week name\n",
        "df_ny['start_day_of_week'] = df_ny['INCIDENT START DATE'].dt.strftime('%A')\n",
        "\n",
        "# Add a column for time\n",
        "df_ny['start_time'] = df_ny['INCIDENT START DATE'].dt.strftime('%H:%M:%S')\n",
        "\n",
        "# df_ny.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ny['DUTY OFFIC NOTIFDATE'] = pd.to_datetime(df_ny['DUTY OFFIC NOTIFDATE'])\n",
        "\n",
        "# Add a column for year \n",
        "df_ny['notif_year'] = df_ny['DUTY OFFIC NOTIFDATE'].dt.strftime('%Y')\n",
        "\n",
        "# Add a column for month name\n",
        "df_ny['notif_month'] = df_ny['DUTY OFFIC NOTIFDATE'].dt.strftime('%B')\n",
        "\n",
        "# Add a column for day of week name\n",
        "df_ny['notif_day_of_week'] = df_ny['DUTY OFFIC NOTIFDATE'].dt.strftime('%A')\n",
        "\n",
        "# Add a column for time\n",
        "df_ny['notif_time'] = df_ny['DUTY OFFIC NOTIFDATE'].dt.strftime('%H:%M:%S')\n",
        "\n",
        "# df_ny.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ny['INCIDENT CLOSED DATE/TIME'] = pd.to_datetime(df_ny['INCIDENT CLOSED DATE/TIME'])\n",
        "\n",
        "# Add a column for year \n",
        "df_ny['closed_year'] = df_ny['INCIDENT CLOSED DATE/TIME'].dt.strftime('%Y')\n",
        "\n",
        "# Add a column for month name\n",
        "df_ny['closed_month'] = df_ny['INCIDENT CLOSED DATE/TIME'].dt.strftime('%B')\n",
        "\n",
        "# Add a column for day of week name\n",
        "df_ny['closed_day_of_week'] = df_ny['INCIDENT CLOSED DATE/TIME'].dt.strftime('%A')\n",
        "\n",
        "# Add a column for time\n",
        "df_ny['closed_time'] = df_ny['INCIDENT CLOSED DATE/TIME'].dt.strftime('%H:%M:%S')\n",
        "\n",
        "# df_ny.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can drop the INCIDENT START TIME and DUTY OFFIC NOTIFDATE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ny = df_ny.drop(columns=[\"INCIDENT START DATE\", \"DUTY OFFIC NOTIFDATE\", \"INCIDENT CLOSED DATE/TIME\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Wildland NYS Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wildland = pd.read_csv(\"Wildland_Search_and_Rescue_Missions_by_NYS_Forest_Rangers__Beginning_2012.csv\")\n",
        "df_wildland.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert date and time columns to datetime objects\n",
        "df_wildland['TOTAL INCIDENT TIME'] = pd.to_datetime(df_wildland['INCIDENT CLOSED DATE'] + ' ' + df_wildland['INCIDENT CLOSED TIME']) - pd.to_datetime(df_wildland['INCIDENT START DATE'] + ' ' + df_wildland['INCIDENT START TIME'])\n",
        "df_wildland['TOTAL INCIDENT TIME'] = df_wildland['TOTAL INCIDENT TIME'] / pd.Timedelta(hours=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wildland['INCIDENT CLOSED DATE'] = pd.to_datetime(df_wildland['INCIDENT CLOSED DATE'])\n",
        "df_wildland['INCIDENT CLOSED TIME'] = pd.to_datetime(df_wildland['INCIDENT CLOSED TIME'])\n",
        "\n",
        "# Add a column for year \n",
        "df_wildland['closed_year'] = df_wildland['INCIDENT CLOSED DATE'].dt.strftime('%Y')\n",
        "\n",
        "# Add a column for month name\n",
        "df_wildland['closed_month'] = df_wildland['INCIDENT CLOSED DATE'].dt.strftime('%B')\n",
        "\n",
        "# Add a column for day of week name\n",
        "df_wildland['closed_day_of_week'] = df_wildland['INCIDENT CLOSED DATE'].dt.strftime('%A')\n",
        "\n",
        "# Add a column for time\n",
        "df_wildland['closed_time'] = df_wildland['INCIDENT CLOSED TIME'].dt.strftime('%H:%M:%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wildland['INCIDENT START DATE'] = pd.to_datetime(df_wildland['INCIDENT START DATE'])\n",
        "df_wildland['INCIDENT START TIME'] = pd.to_datetime(df_wildland['INCIDENT START TIME'])\n",
        "\n",
        "# Add a column for year \n",
        "df_wildland['start_year'] = df_wildland['INCIDENT START DATE'].dt.strftime('%Y')\n",
        "\n",
        "# Add a column for month name\n",
        "df_wildland['start_month'] = df_wildland['INCIDENT START DATE'].dt.strftime('%B')\n",
        "\n",
        "# Add a column for day of week name\n",
        "df_wildland['start_day_of_week'] = df_wildland['INCIDENT START DATE'].dt.strftime('%A')\n",
        "\n",
        "# Add a column for time\n",
        "df_wildland['start_time'] = df_wildland['INCIDENT START TIME'].dt.strftime('%H:%M:%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wildland = df_wildland.drop(columns=[\"INCIDENT START DATE\", \"INCIDENT START TIME\", \"INCIDENT CLOSED DATE\", \"INCIDENT CLOSED TIME\",])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wildland = df_wildland.drop(columns=[\"Location\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge Wildland & NY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wildland.rename(columns={'LOCATION FOUND LATITUDE': 'LOCATION FOUND LAT', 'LOCATION FOUND LONGITUDE': 'LOCATION FOUND LONG', 'TOTAL INCIDENT TIME': 'TOTAL HOURS ISD'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ny.rename(columns={'LAST KNOWN POINTCOUNTY': 'LAST KNOWN POINT COUNTY', 'LAST KNOWN POINTSTATELANDNAME': 'LAST KNOWN POINT STATE LAND NAME', 'INCIDENT LASTKNOWNPOINT MUNI': 'LAST KNOWN POINT MUNICIPALITY'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find overlapping columns between Wildland and NY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Find overlapping and non-overlapping column names\n",
        "overlapping_cols = list(set(df_ny.columns) & set(df_wildland.columns))\n",
        "non_overlapping_cols_df_ny = list(set(df_ny.columns) - set(df_wildland.columns))\n",
        "non_overlapping_cols_df_wildland = list(set(df_wildland.columns) - set(df_ny.columns))\n",
        "\n",
        "non_overlapping_df1 = df_ny[non_overlapping_cols_df_ny]\n",
        "non_overlapping_df2 = df_wildland[non_overlapping_cols_df_wildland]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find any cases that are in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Find if any rows overlap using 'INCIDENT NUMBER'\n",
        "# Find common 'INCIDENT NUMBER' values between the two dataframes\n",
        "common_incident_numbers = set(df_ny['INCIDENT NUMBER']).intersection(df_wildland['INCIDENT NUMBER'])\n",
        "\n",
        "# Filter rows in df1 and df2 that have common 'INCIDENT NUMBER' values\n",
        "common_rows_df1 = df_ny[df_ny['INCIDENT NUMBER'].isin(common_incident_numbers)]\n",
        "common_rows_df2 = df_wildland[df_wildland['INCIDENT NUMBER'].isin(common_incident_numbers)]\n",
        "\n",
        "if common_rows_df1.empty or common_rows_df2.empty:\n",
        "    print(\"No common rows found.\")\n",
        "else:\n",
        "    print(\"Common rows found in df1:\")\n",
        "    print(common_rows_df1)\n",
        "    # print(\"\\nCommon rows found in df2:\")\n",
        "    # print(common_rows_df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = pd.merge(df_ny, df_wildland, on='INCIDENT NUMBER', how='outer')\n",
        "\n",
        "# Identify columns unique to each dataframe\n",
        "columns_unique_to_df1 = [col for col in df_ny.columns if col not in df_wildland.columns]\n",
        "columns_unique_to_df2 = [col for col in df_wildland.columns if col not in df_ny.columns]\n",
        "\n",
        "# Replace NaN values with 'NA' and concatenate unique columns\n",
        "merged_df[columns_unique_to_df1] = merged_df[columns_unique_to_df1].fillna('NA')\n",
        "merged_df[columns_unique_to_df2] = merged_df[columns_unique_to_df2].fillna('NA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = merged_df[[col for col in merged_df.columns if not col.endswith('_y')]]\n",
        "\n",
        "merged_df.columns = [col.replace('_x', '') for col in merged_df.columns]\n",
        "\n",
        "print(\"DataFrame after removing '_x' suffix from column names:\")\n",
        "merged_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "non_unique_rows = merged_df[merged_df.duplicated('INCIDENT NUMBER', keep=False)]\n",
        "unique_rows = non_unique_rows.drop_duplicates('INCIDENT NUMBER')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = merged_df.dropna(subset=['closed_day_of_week'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mission_level_columns = list(merged_df.columns)  # Add all 65 column names here\n",
        "values_to_drop = ['SUBJECT AGE', 'SUBJECT GENDER', 'SUBJECT NUMBER', 'INCIDENT NUMBER']\n",
        "mission_level_columns = [x for x in mission_level_columns if x not in values_to_drop]\n",
        "\n",
        "merged_df['SUBJECT AGE'] = pd.to_numeric(merged_df['SUBJECT AGE'], errors='coerce')\n",
        "\n",
        "# Create a dictionary for aggregation\n",
        "aggregation_dict = {\n",
        "    'SUBJECT NUMBER': 'max',  # Maximum subject ID in the group\n",
        "    'SUBJECT AGE': ['min', 'max'],  # Minimum and maximum subject age in the group\n",
        "    'SUBJECT GENDER': [lambda x: (x == 'F').sum(), lambda x: (x == 'M').sum()]#,  # Number of females in the group\n",
        "    # Number of males in the group (total subjects - number of females)\n",
        "    #'SUBJECT GENDER': lambda x: (x == 'M').sum()\n",
        "}\n",
        "\n",
        "# Group the DataFrame by 'INCIDENT NUMBER' and aggregate the information\n",
        "aggregated_df = merged_df.groupby('INCIDENT NUMBER').agg(aggregation_dict)\n",
        "\n",
        "# Retrieve values for mission-level columns from the original DataFrame\n",
        "for column in merged_df.columns:\n",
        "    if column not in aggregation_dict:\n",
        "        aggregated_df[column] = merged_df.groupby('INCIDENT NUMBER')[column].first()\n",
        "\n",
        "# Rename the aggregated columns for clarity\n",
        "aggregated_df.columns = ['Total Subjects', 'Min Subject Age', 'Max Subject Age', 'Gender F Count', 'Gender M Count', 'Incident Number agg'] + mission_level_columns\n",
        "\n",
        "# Reset index to make 'INCIDENT NUMBER' a column instead of an index\n",
        "aggregated_df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert 'SUBJECT AGE' column to numeric, coercing errors to NaN\n",
        "merged_df['SUBJECT AGE'] = pd.to_numeric(merged_df['SUBJECT AGE'], errors='coerce')\n",
        "\n",
        "# Group the DataFrame by 'INCIDENT NUMBER' and aggregate the information\n",
        "aggregated_df = merged_df.groupby('INCIDENT NUMBER').agg({\n",
        "    'SUBJECT NUMBER': 'max',  # Maximum subject ID in the group\n",
        "    'SUBJECT AGE': ['min', 'max'],  # Minimum and maximum subject age in the group\n",
        "    'SUBJECT GENDER': [lambda x: (x == 'F').sum(), lambda x: (x == 'M').sum()]  # Number of females, males in the group\n",
        "})\n",
        "\n",
        "# Rename the aggregated columns for clarity\n",
        "aggregated_df.columns = ['Total Subjects', 'Min Subject Age', 'Max Subject Age', 'Gender F Count', 'Gender M Count']\n",
        "\n",
        "# Reset index to make 'INCIDENT NUMBER' a column instead of an index\n",
        "aggregated_df.reset_index(inplace=True)\n",
        "\n",
        "aggregated_df['Total Subjects'] = pd.to_numeric(aggregated_df['Total Subjects'], errors='coerce')\n",
        "aggregated_df['Gender F Count'] = pd.to_numeric(aggregated_df['Gender F Count'], errors='coerce')\n",
        "aggregated_df['Gender M Count'] = pd.to_numeric(aggregated_df['Gender M Count'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_merged_df = merged_df.drop_duplicates(subset=['INCIDENT NUMBER'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_merged_df = unique_merged_df.drop(columns=[\"SUBJECT NUMBER\", \"SUBJECT AGE\", \"SUBJECT GENDER\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge merged_df and aggregated_df on 'INCIDENT NUMBER' column\n",
        "df_sar = pd.merge(unique_merged_df, aggregated_df, on='INCIDENT NUMBER', how='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bin Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check for quantitative variables to bin: \n",
        "MIN TEMP DEG F, MAX TEMP DEG F, MIN SNOW DEPTH IN, MAX SNOW DEPTH IN, LOCATION FOUND LAT, LOCATION FOUND LONG, LOCATION FOUND ELEVATION, TOTAL HOURS ISD, TOTAL HOURS DON, notif_time, closed_time, Total Subjects, Min Subject Age, Max Subject Age, Distinct Equipment Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Use df.head() to display the DataFrame with all columns\n",
        "print(df_sar.head())\n",
        "\n",
        "# After displaying, you may want to reset the option back to its default value\n",
        "pd.reset_option('display.max_columns')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Num Rangers\n",
        "Bin Number of Rangers Involved into four groups: '1-4', '5-10', '11-20', '21+'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [1, 5, 11, 21, float('inf')]  # Define the bin edges: 0-4, 5-10, 11+\n",
        "\n",
        "labels = ['1-4', '5-10', '11-20', '21+']\n",
        "\n",
        "df_sar['NUMBER RANGERS BINNED'] = pd.cut(df_sar['NUMBER OF RANGERS INVOLVED'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['NUMBER OF RANGERS INVOLVED', 'NUMBER RANGERS BINNED']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Min Temp\n",
        "Check Min Temp Deg F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['MIN TEMP DEG F'] = pd.to_numeric(df_sar['MIN TEMP DEG F'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['MIN TEMP DEG F'], bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Min Temp Deg F into 6 groups:\n",
        "'-30F to 0F', '1F to 20F', '21F to 40F', '41F to 60F', '61F to 80F', '81F to 100F'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [-30, 1, 21, 41, 61, 81, float('inf')]  # Define the bin edges: 0-4, 5-10, 11+\n",
        "\n",
        "labels = ['-30F to 0F', '1F to 20F', '21F to 40F', '41F to 60F', '61F to 80F', '81F to 100F']\n",
        "\n",
        "df_sar['MIN TEMP BINNED'] = pd.cut(df_sar['MIN TEMP DEG F'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['MIN TEMP BINNED', 'MIN TEMP DEG F']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Max Temp\n",
        "Check Max Temp Deg F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['MAX TEMP DEG F'] = pd.to_numeric(df_sar['MAX TEMP DEG F'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['MAX TEMP DEG F'], bins=20)\n",
        "plt.xlim(-30, 130)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Max Temp Deg F into 7 groups:\n",
        "'-30F to 30F', '31F to 50F', '51F to 60F', '61F to 70F', '71F to 80F', '81F to 90F', '91F to 120F'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [-30, 31, 51, 61, 71, 81, 91, float('inf')]  # Define the bin edges: 0-4, 5-10, 11+\n",
        "\n",
        "labels = ['-30F to 30F', '31F to 50F', '51F to 60F', '61F to 70F', '71F to 80F', '81F to 90F', '91F to 120F']\n",
        "\n",
        "df_sar['MAX TEMP BINNED'] = pd.cut(df_sar['MAX TEMP DEG F'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['MAX TEMP BINNED', 'MAX TEMP DEG F']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Min Snow Depth\n",
        "Check Min Snow Depth In"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['MIN SNOW DEPTH IN'] = pd.to_numeric(df_sar['MIN SNOW DEPTH IN'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['MIN SNOW DEPTH IN'], bins=5)\n",
        "plt.xlim(-1, 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Min Snow Depth In into 5 groups: \n",
        "'0 IN', '1-5 IN', '6-10 IN', '11-20 IN', '21-60 IN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [-1, 1, 6, 11, 21, float('inf')]\n",
        "\n",
        "labels = ['0 IN', '1-5 IN', '6-10 IN', '11-20 IN', '21-60 IN']\n",
        "\n",
        "df_sar['MIN SNOW DEPTH BINNED'] = pd.cut(df_sar['MIN SNOW DEPTH IN'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['MIN SNOW DEPTH BINNED', 'MIN SNOW DEPTH IN']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Max Snow Depth\n",
        "Check Max Snow Depth In"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['MAX SNOW DEPTH IN'] = pd.to_numeric(df_sar['MAX SNOW DEPTH IN'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['MAX SNOW DEPTH IN'], bins=15)\n",
        "plt.xlim(-1, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Max Snow Depth In into 5 groups:\n",
        "'0 IN', '1-5 IN', '6-10 IN', '11-20 IN', '21-80 IN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [-1, 1, 6, 11, 21, float('inf')]\n",
        "\n",
        "labels = ['0 IN', '1-5 IN', '6-10 IN', '11-20 IN', '21-80 IN']\n",
        "\n",
        "df_sar['MAX SNOW DEPTH BINNED'] = pd.cut(df_sar['MAX SNOW DEPTH IN'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['MAX SNOW DEPTH BINNED', 'MAX SNOW DEPTH IN']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loc Found Lat\n",
        "Check Location Found Lat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['LOCATION FOUND LAT'] = pd.to_numeric(df_sar['LOCATION FOUND LAT'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['LOCATION FOUND LAT'], bins=15)\n",
        "plt.xlim(40, 45.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Location Found Lat into four groups: '40 - 42', '42 - 43', '43 - 44', '44 - 45'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 42, 43, 44, float('inf')]\n",
        "\n",
        "labels = ['40 - 42', '42 - 43', '43 - 44', '44 - 45']\n",
        "\n",
        "df_sar['LOCATION FOUND LAT BINNED'] = pd.cut(df_sar['LOCATION FOUND LAT'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['LOCATION FOUND LAT BINNED', 'LOCATION FOUND LAT']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loc Found Long\n",
        "Check Location Found Long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['LOCATION FOUND LONG'] = pd.to_numeric(df_sar['LOCATION FOUND LONG'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['LOCATION FOUND LONG'], bins=15)\n",
        "plt.xlim(-80, -71)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Location Found Long into five groups: '-80 to -77', '-77 to -75', '-75 to -74', '-74 to -73.5', '-73.5 to -71'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [-80, -77, -75, -74, -73.5, float('inf')]\n",
        "\n",
        "labels = ['-80 to -77', '-77 to -75', '-75 to -74', '-74 to -73.5', '-73.5 to -71']\n",
        "\n",
        "df_sar['LOCATION FOUND LONG BINNED'] = pd.cut(df_sar['LOCATION FOUND LONG'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['LOCATION FOUND LONG BINNED', 'LOCATION FOUND LONG']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loc Found Elevation\n",
        "Check Location Found Elevation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['LOCATION FOUND ELEVATION'] = pd.to_numeric(df_sar['LOCATION FOUND ELEVATION'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['LOCATION FOUND ELEVATION'], bins=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Location Found Elevation to 5 groups: '0 - 1000', '1000 - 2000', '2000 - 3000', '3000 - 4000', '4000 - 5500'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 1000, 2000, 3000, 4000, float('inf')]\n",
        "\n",
        "labels = ['0 - 1000', '1000 - 2000', '2000 - 3000', '3000 - 4000', '4000 - 5500']\n",
        "\n",
        "df_sar['LOCATION FOUND ELEVATION BINNED'] = pd.cut(df_sar['LOCATION FOUND ELEVATION'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar.loc[:,['LOCATION FOUND ELEVATION BINNED', 'LOCATION FOUND ELEVATION']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Total Hours ISD\n",
        "Check Total Hours ISD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['TOTAL HOURS ISD'] = pd.to_numeric(df_sar['TOTAL HOURS ISD'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['TOTAL HOURS ISD'], bins=20)\n",
        "plt.xlim(16000, 800000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Total Hours ISD to 9 groups: '0-1 hrs', '1-2 hrs', '2-3 hrs', '3-4 hrs', '4-5 hrs', '5-10 hrs', '10-30 hrs', \n",
        "          '30-100', '100-800000 hrs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 1, 2, 3, 4, 5, 10, 30, 100, float('inf')]\n",
        "\n",
        "labels = ['0-1 hrs', '1-2 hrs', '2-3 hrs', '3-4 hrs', '4-5 hrs', '5-10 hrs', '10-30 hrs', \n",
        "          '30-100', '100-800000 hrs']\n",
        "\n",
        "df_sar['TOTAL HOURS ISD BINNED'] = pd.cut(df_sar['TOTAL HOURS ISD'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['TOTAL HOURS ISD BINNED'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Total Hours DON\n",
        "Check Total Hours DON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['TOTAL HOURS DON'] = pd.to_numeric(df_sar['TOTAL HOURS DON'], errors='coerce').astype('float')\n",
        "plt.hist(df_sar['TOTAL HOURS DON'], bins=20)\n",
        "# plt.xlim(16000, 800000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Total Hours Don in 9 groups: '0-1 hrs', '1-2 hrs', '2-3 hrs', '3-4 hrs', '4-5 hrs', '5-10 hrs', '10-30 hrs', \n",
        "          '30-100', '100-800000 hrs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 1, 2, 3, 4, 5, 10, 30, 100, float('inf')]\n",
        "\n",
        "labels = ['0-1 hrs', '1-2 hrs', '2-3 hrs', '3-4 hrs', '4-5 hrs', '5-10 hrs', '10-30 hrs', \n",
        "          '30-100', '100-800000 hrs']\n",
        "\n",
        "df_sar['TOTAL HOURS DON BINNED'] = pd.cut(df_sar['TOTAL HOURS DON'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['TOTAL HOURS DON BINNED'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bin Start Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to map time to three-hour intervals\n",
        "def map_to_interval(time_str):\n",
        "    # Extract hours from the time string\n",
        "    hour = int(time_str[:2])\n",
        "    \n",
        "    # Determine the interval based on the hour\n",
        "    intervals = ['0-4am', '4-8am', '8am-12pm', '12-4pm', '4-8pm', '8pm-12am']\n",
        "    interval_index = hour // 4\n",
        "    return intervals[interval_index]\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['start_time_interval'] = df_sar['start_time'].apply(map_to_interval)\n",
        "\n",
        "print(df_sar[['start_time', 'start_time_interval']])\n",
        "\n",
        "# plt.hist(df_sar['notif_time'], bins=20)\n",
        "# plt.xlim(16000, 800000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gender F Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(df_sar['Gender F Count']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Gender F Count in 6 groups: '0 F', '1 F', '2 F', '3 F', '4 F', '5+ F'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 1, 2, 3, 4, 5, float('inf')]\n",
        "\n",
        "labels = ['0 F', '1 F', '2 F', '3 F', '4 F', '5+ F']\n",
        "\n",
        "df_sar['Gender F Count Binned'] = pd.cut(df_sar['Gender F Count'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['Gender F Count Binned'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gender M Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['Gender M Count'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Gender M Count in 6 groups: '0 M', '1 M', '2 M', '3 M', '4 M', '5+ M'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 1, 2, 3, 4, 5, float('inf')]\n",
        "\n",
        "labels = ['0 M', '1 M', '2 M', '3 M', '4 M', '5+ M']\n",
        "\n",
        "df_sar['Gender M Count Binned'] = pd.cut(df_sar['Gender M Count'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['Gender M Count Binned'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prop M Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['Prop M Subjects'] = df_sar['Gender M Count'] / (df_sar['Gender M Count'] + df_sar['Gender F Count'])\n",
        "plt.hist(df_sar['Prop M Subjects']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 0.01, 0.5, 0.51, 1, float('inf')]\n",
        "\n",
        "labels = ['0% M', '1-49% M', '50% M', '51-99% M', '100% M']\n",
        "df_sar['Prop M Subjects Binned'] = pd.cut(df_sar['Prop M Subjects'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['Prop M Subjects Binned'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Total Subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['Total Subjects'] = df_sar['Gender M Count'] + df_sar['Gender F Count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Total Subjects in 6 groups: '0', '1', '2', '3', '4', '5+'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 1, 2, 3, 4, 5, float('inf')]\n",
        "\n",
        "labels = ['0', '1', '2', '3', '4', '5+']\n",
        "\n",
        "df_sar['Total Subjects Binned'] = pd.cut(df_sar['Total Subjects'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['Total Subjects Binned'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Min Subject Age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Min Subject Age in 8 groups: '0', '10', '20', '30', '40', '50', '60', '70+'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['Min Subject Age'].value_counts()\n",
        "plt.hist(df_sar['Min Subject Age'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 10, 20, 30, 40, 50, 60, 70, float('inf')]\n",
        "\n",
        "labels = ['0', '10', '20', '30', '40', '50', '60', '70+']\n",
        "\n",
        "df_sar['Min Subject Age Binned'] = pd.cut(df_sar['Min Subject Age'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['Min Subject Age Binned'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Max Subject Age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bin Max Subject Age in 8 groups: '0', '10', '20', '30', '40', '50', '60', '70+'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['Max Subject Age'].value_counts()\n",
        "plt.hist(df_sar['Max Subject Age'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = [0, 10, 20, 30, 40, 50, 60, 70, float('inf')]\n",
        "\n",
        "labels = ['0', '10', '20', '30', '40', '50', '60', '70+']\n",
        "\n",
        "df_sar['Max Subject Age Binned'] = pd.cut(df_sar['Max Subject Age'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_sar['Max Subject Age Binned'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change floats to strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['INCIDENT REGION'] = df_sar['INCIDENT REGION'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reclassify Categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Equipment Ind to df_sar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unique equipment values\n",
        "unique_equipment_list = set(';'.join(df_sar['EQUIPMENT USED']).split(';'))\n",
        "unique_equipment = {equipment.strip() for equipment in unique_equipment_list}\n",
        "\n",
        "# Create dummy columns\n",
        "for equipment in unique_equipment:\n",
        "    df_sar[equipment.strip() + ' IND'] = df_sar['EQUIPMENT USED'].apply(lambda x: 'Yes' if equipment in x else 'No')\n",
        "\n",
        "df_sar.rename(columns={'NA IND': 'NA EQUIPMENT IND'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Technique Ind to df_sar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['TECHNIQUE'] = df_sar['TECHNIQUE'].astype(str)\n",
        "# Get unique equipment values\n",
        "unique_technique_list = set(';'.join(df_sar['TECHNIQUE']).split(';'))\n",
        "unique_technique = {technique.strip() for technique in unique_technique_list}\n",
        "\n",
        "# Create dummy columns\n",
        "for technique in unique_technique:\n",
        "    df_sar[technique.strip() + ' IND'] = df_sar['TECHNIQUE'].apply(lambda x: 'Yes' if technique in x else 'No')\n",
        "\n",
        "\n",
        "df_sar.rename(columns={'nan IND': 'NA TECHNIQUE IND'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine Some Techniques/Equipment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine ATV/UTV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_atv_utv(row):\n",
        "    if row['ATV/UTV IND'] == 'Yes' or row['Van IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Van/ATV/UTVIND'] = df_sar.apply(lambda row: check_atv_utv(row), axis=1)\n",
        "df_sar = df_sar.drop(columns=[\"ATV/UTV IND\", 'Van IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Technical Rope/High Angle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_technical_rope(row):\n",
        "    if row['Technical Rope IND'] == 'Yes' or row['High Angle IND'] == 'Yes' or row['Technical rope gear IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Technical Rope/High Angle IND'] = df_sar.apply(lambda row: check_technical_rope(row), axis=1)\n",
        "df_sar = df_sar.drop(columns=['Technical Rope IND', 'High Angle IND', 'Technical rope gear IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Aircraft/Helicopter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_aircraft_heli(row):\n",
        "    if row['Aircraft evacuation IND'] == 'Yes' or row['Helicopter IND'] == 'Yes' or row['Fixed Wing Aircraft IND'] == 'Yes' or row['UAV IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Aircraft/Helicopter/UAV IND'] = df_sar.apply(lambda row: check_aircraft_heli(row), axis=1)\n",
        "df_sar = df_sar.drop(columns=['Aircraft evacuation IND', 'Helicopter IND', 'Fixed Wing Aircraft IND', 'UAV IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Boat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_boat(row):\n",
        "    if row['Boat/canoe/watercraft IND'] == 'Yes' or row['Swiftwater raft IND'] == 'Yes' or row['Swiftwater IND'] == 'Yes' or row['Vehicle/boat evacuation IND'] == 'Yes' or row['IRV IND'] == 'Yes' or row['Airboat IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Boat/Airboat IND'] = df_sar.apply(lambda row: check_boat(row), axis=1)\n",
        "df_sar = df_sar.drop(columns=['Boat/canoe/watercraft IND', 'Swiftwater raft IND', 'Swiftwater IND', 'Vehicle/boat evacuation IND', 'IRV IND', 'Airboat IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_treatment(row):\n",
        "    if row['TREATMENT PROVIDED BY RANGERS'] == 'Life-Saving Measures' or row['TREATMENT PROVIDED BY OTHERS'] == 'Life-Saving Measures':\n",
        "        return 'Life-Saving Measures'\n",
        "    if row['TREATMENT PROVIDED BY RANGERS'] == 'Basic First Aid' or row['TREATMENT PROVIDED BY OTHERS'] == 'Basic First Aid':\n",
        "        return 'Basic First Aid'\n",
        "    if row['TREATMENT PROVIDED BY RANGERS'] == 'Subject Declined Treatment' or row['TREATMENT PROVIDED BY OTHERS'] == 'Subject Declined Treatment':\n",
        "        return 'Subject Declined Treatment'\n",
        "    if row['TREATMENT PROVIDED BY RANGERS'] == '' and row['TREATMENT PROVIDED BY OTHERS'] == 'NA':\n",
        "        return 'NA'\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Treatment Provided'] = df_sar.apply(lambda row: check_treatment(row), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Weather Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_weather(row):\n",
        "    if row['WEATHER IND STORMY'] == 'Yes' or row['WEATHER IND RAIN'] == 'Yes':\n",
        "        return 'Rain/Storm'\n",
        "    elif row['WEATHER IND SNOW'] == 'Yes':\n",
        "        return 'Snow'\n",
        "    elif row['WEATHER IND OVERCAST'] == 'Yes' or row['WEATHER IND FOG'] == 'Yes':\n",
        "        return 'Overcast/Fog'\n",
        "    elif row['WEATHER IND WIND'] == 'Yes':\n",
        "        return 'Wind'\n",
        "    elif row['WEATHER IND CLEAR'] == 'Yes':\n",
        "        return 'Clear'\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Weather'] = df_sar.apply(lambda row: check_weather(row), axis=1)\n",
        "df_sar = df_sar.drop(columns=['WEATHER IND CLEAR', 'WEATHER IND WIND', 'WEATHER IND OVERCAST', 'WEATHER IND FOG', 'WEATHER IND SNOW', 'WEATHER IND STORMY', 'WEATHER IND RAIN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Terrain Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_terrain(row):\n",
        "    terrain = []\n",
        "    if row['TECHNICAL TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Technical')\n",
        "    elif row['MOUNTAIN TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Mountain')\n",
        "    elif row['HILY TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Hilly')\n",
        "    elif row['FLAT TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Flat')\n",
        "    else:\n",
        "        terrain.append('NA')\n",
        "    return ', '.join(terrain)\n",
        "\n",
        "# Create new column 'WEATHER IND'\n",
        "df_sar['TERRAIN'] = df_sar.apply(get_terrain, axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['TECHNICAL TERRAIN IND', 'MOUNTAIN TERRAIN IND', 'HILY TERRAIN IND', 'FLAT TERRAIN IND'])\n",
        "df_sar.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['TERRAIN'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_terrain_type(row):\n",
        "    terrain = []\n",
        "    if row['SNOW TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Snow')\n",
        "    elif row['WOODS TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Woods')\n",
        "    elif row['MARSH/SWAMP TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Marsh/Swamp')\n",
        "    elif row['ROCK TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Rock')\n",
        "    elif row['OPEN TERRAIN IND'] == 'Yes':\n",
        "        terrain.append('<=Open')\n",
        "    else:\n",
        "        terrain.append('NA')\n",
        "    return ', '.join(terrain)\n",
        "\n",
        "# Create new column 'WEATHER IND'\n",
        "df_sar['TERRAIN TYPE'] = df_sar.apply(get_terrain_type, axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['SNOW TERRAIN IND', 'MARSH/SWAMP TERRAIN IND', 'ROCK TERRAIN IND', 'WOODS TERRAIN IND', 'OPEN TERRAIN IND'])\n",
        "df_sar.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar['TERRAIN TYPE'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Land Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_landclass(row):\n",
        "    if row['LAST KNOWN POINT LAND CLASS'] in ('Forest Preserve', 'State Forests', 'OPRHP State Land', 'Conservation Easement', 'Wildlife Management Area', 'DEC Campground'):\n",
        "        return 'Protected Area'\n",
        "    elif row['LAST KNOWN POINT LAND CLASS'] in ('Private Land', 'Other Municipal Land', 'Other DEC Land', 'County Land', 'Federal Land', \"Native People's Land\"):\n",
        "        return 'Private Land'\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Land Class'] = df_sar.apply(lambda row: check_landclass(row), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Adriondack and Catskill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_park(row):\n",
        "    if row['INCIDENT ADIRONDACK PARK'] == 'Yes':\n",
        "        return 'Adirondack'\n",
        "    if row['INCIDENT CATSKILL PARK'] == 'Yes':\n",
        "        return 'Catskill'\n",
        "    if row['INCIDENT CATSKILL PARK'] == 'No' and row['INCIDENT ADIRONDACK PARK'] == 'No':\n",
        "        return 'Neither'\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Incident Park'] = df_sar.apply(lambda row: check_park(row), axis=1)\n",
        "df_sar = df_sar.drop(columns=['INCIDENT CATSKILL PARK', 'INCIDENT ADIRONDACK PARK'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Reasons for Being Lost Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_reason_list = set(';'.join(df_sar['REASON FOR BEING LOST']).split(';'))\n",
        "unique_reason = {reason.strip() for reason in unique_reason_list}\n",
        "\n",
        "#Create dummy columns\n",
        "for reason in unique_reason:\n",
        "    df_sar[reason.strip() + ' IND'] = df_sar['REASON FOR BEING LOST'].apply(lambda x: 'Yes' if reason in x else 'No')\n",
        "\n",
        "# Drop second NA equipment column\n",
        "#df_sar.drop(columns=['NA IND'], inplace=True)#\n",
        "\n",
        "df_sar.rename(columns={'NA IND': 'NA REASON IND'}, inplace=True)\n",
        "\n",
        "\n",
        "# Calculate number of distinct equipment used - deleted 'NA EQUIPMENT IND', from columns=\n",
        "df_sar.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Reasons for Being Lost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_reason_prep(row):\n",
        "    if row['Poor supervision IND'] == 'Yes' or row['Poor map/No map IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "    \n",
        "def get_reason_exec(row):\n",
        "    if row['Misjudged time/distance IND'] == 'Yes' or row['Took short cut IND'] == 'Yes' or row['Accidental separation IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Poor/Inadequate Planning'] = df_sar.apply(lambda row: get_reason_prep(row), axis=1)\n",
        "df_sar['Poor/Inadequate Execution'] = df_sar.apply(lambda row: get_reason_exec(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Poor supervision IND', 'Poor map/No map IND', 'Misjudged time/distance IND', 'Took short cut IND', 'Accidental separation IND'])\n",
        "df_sar.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Activity Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_activity_list = set(';'.join(df_sar['ACTIVITY']).split(';'))\n",
        "unique_activity = {activity.strip() for activity in unique_activity_list}\n",
        "\n",
        "#Create dummy columns\n",
        "for activity in unique_activity:\n",
        "    df_sar[activity.strip() + ' ACTIVITY IND'] = df_sar['ACTIVITY'].apply(lambda x: 'Yes' if activity in x else 'No')\n",
        "\n",
        "# Drop second NA equipment column\n",
        "#df_sar.drop(columns=['NA IND'], inplace=True)#\n",
        "\n",
        "df_sar.rename(columns={'NA IND': 'NA ACTIVITY IND'}, inplace=True)\n",
        "\n",
        "\n",
        "# Calculate number of distinct equipment used - deleted 'NA EQUIPMENT IND', from columns=\n",
        "df_sar.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Actions Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_actions_list = set(';'.join(df_sar['ACTIONS']).split(';'))\n",
        "unique_actions = {action.strip() for action in unique_actions_list}\n",
        "\n",
        "#Create dummy columns\n",
        "for action in unique_actions:\n",
        "    df_sar[action.strip() + ' ACTION IND'] = df_sar['ACTIONS'].apply(lambda x: 'Yes' if action in x else 'No')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Route Followed IND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_route_list = set(';'.join(df_sar['ROUTE FOLLOWED']).split(';'))\n",
        "unique_route = {route.strip() for route in unique_route_list}\n",
        "\n",
        "#Create dummy columns\n",
        "for route in unique_route:\n",
        "    df_sar[route.strip() + ' ROUTE IND'] = df_sar['ROUTE FOLLOWED'].apply(lambda x: 'Yes' if route in x else 'No')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Detectability Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_detect_list = set(';'.join(df_sar['DETECTABILITY']).split(';'))\n",
        "unique_detect = {detect.strip() for detect in unique_detect_list}\n",
        "print(unique_detect)\n",
        "#Create dummy columns\n",
        "for detect in unique_detect:\n",
        "    df_sar[detect.strip() + ' DETECT IND'] = df_sar['DETECTABILITY'].apply(lambda x: 'Yes' if detect in x else 'No')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Situation Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_situation_list = set(';'.join(df_sar['SITUATION']).split(';'))\n",
        "unique_situation = {situation.strip() for situation in unique_situation_list}\n",
        "print(unique_situation)\n",
        "#Create dummy columns\n",
        "for situation in unique_situation:\n",
        "    df_sar[situation.strip() + ' SITUATION IND'] = df_sar['SITUATION'].apply(lambda x: 'Yes' if situation in x else 'No')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Contributing Factor Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_factor_list = set(';'.join(df_sar['CONTRIBUTING FACTOR']).split(';'))\n",
        "unique_factor = {factor.strip() for factor in unique_factor_list}\n",
        "print(unique_factor)\n",
        "#Create dummy columns\n",
        "for factor in unique_factor:\n",
        "    df_sar[factor.strip() + ' CONTR FACTOR IND'] = df_sar['CONTRIBUTING FACTOR'].apply(lambda x: 'Yes' if factor in x else 'No')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_action_surv(row):\n",
        "    if row['Found food ACTION IND'] == 'Yes' or row['Sought/built shelter ACTION IND'] == 'Yes':\n",
        "        return 'Found food/built shelter'\n",
        "    if row['Conserved energy ACTION IND'] == 'Yes':\n",
        "        return 'Conserved energy'\n",
        "    else:\n",
        "        return 'Neither'   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Survival Action IND'] = df_sar.apply(lambda row: get_action_surv(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Found food ACTION IND', 'Sought/built shelter ACTION IND', 'Conserved energy ACTION IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Snowmobile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_snowmobile(row):\n",
        "    if row['Snowmobile IND'] == 'Yes' or row['Snowmobile ACTIVITY IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Snowmobile IND'] = df_sar.apply(lambda row: check_snowmobile(row), axis=1)\n",
        "df_sar = df_sar.drop(columns=['Snowmobile ACTIVITY IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Conscious/Unconscious"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_conscious(row):\n",
        "    if row['Conscious DETECT IND'] == 'Yes':\n",
        "        return 'Conscious'\n",
        "    if row['Unconscious DETECT IND'] == 'Yes':\n",
        "        return 'Unconscious'\n",
        "    else:\n",
        "        return 'NA'   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Conscious DETECT IND'] = df_sar.apply(lambda row: get_conscious(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Unconscious DETECT IND', 'NA DETECT IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Uncooperative/Avoidance Ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_avoid(row):\n",
        "    if row['Uncooperative DETECT IND'] == 'Yes' or row['Avoided searchers ACTION IND'] == 'Yes':\n",
        "        return 'Uncooperative/Avoided Searchers'\n",
        "    if row['Cooperative DETECT IND'] == 'Yes':\n",
        "        return 'Cooperative'\n",
        "    else:\n",
        "        return 'Neither'   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Uncooperative/Avoided Searchers IND'] = df_sar.apply(lambda row: get_avoid(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Uncooperative DETECT IND', 'Avoided searchers ACTION IND', 'Cooperative DETECT IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Fugitive/Criminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_criminal(row):\n",
        "    if row['Criminal ACTIVITY IND'] == 'Yes' or row['Fugitive SITUATION IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Fugitive/Criminal IND'] = df_sar.apply(lambda row: get_criminal(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Criminal ACTIVITY IND', 'Fugitive SITUATION IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Activities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_walking_hiking_boating_hunting(row):\n",
        "    if row['Hiking ACTIVITY IND'] == 'Yes' or row['Walking ACTIVITY IND'] == 'Yes':\n",
        "        return 'Walking/Hiking'\n",
        "    if row['Swimming ACTIVITY IND'] == 'Yes' or row['Fishing ACTIVITY IND'] == 'Yes' or row['Boating ACTIVITY IND'] == 'Yes' or row['Whitewater ACTIVITY IND'] == 'Yes':\n",
        "        return 'Water Based'\n",
        "    if row['Hunting ACTIVITY IND'] == 'Yes':\n",
        "        return 'Hunting'\n",
        "    else:\n",
        "        return 'None'   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Hiking/Boating/Hiking ACTIVITY IND'] = df_sar.apply(lambda row: get_walking_hiking_boating(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Swimming ACTIVITY IND', 'Fishing ACTIVITY IND', 'Boating ACTIVITY IND', 'Whitewater ACTIVITY IND', \n",
        "                              'Hiking ACTIVITY IND', 'Walking ACTIVITY IND', 'Hunting ACTIVITY IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Panicked / Built Fire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_panicked(row):\n",
        "    if row['Built fire ACTION IND'] == 'Yes':\n",
        "        return 'Walking/Hiking'\n",
        "    if row['Panicked ACTION IND'] == 'Yes':\n",
        "        return 'Panicked'\n",
        "    if row['Took no action ACTION IND'] == 'Yes':\n",
        "        return 'Took no action'\n",
        "    else:\n",
        "        return 'Neither'   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Action IND'] = df_sar.apply(lambda row: get_panicked(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Panicked ACTION IND', 'Built fire ACTION IND', 'Took no action ACTION IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Lead Organization Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lead_org(row):\n",
        "    if row['LEAD ORGANIZATION NAME'] == 'FBI' or row['LEAD ORGANIZATION NAME'] == 'State Police':\n",
        "        return 'FBI/State Police'\n",
        "    if row['LEAD ORGANIZATION NAME'] == 'DEC Forest Rangers':\n",
        "        return 'DEC Forest Rangers'\n",
        "    else:\n",
        "        return \"Sheriff's/Local Fire/Local Police\"   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['LEAD ORGANIZATION NAME'] = df_sar.apply(lambda row: get_lead_org(row), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine Offroad/Motor Vehicle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_motor(row):\n",
        "    if row['Off road vehicle/ATV ACTIVITY IND'] == 'Yes' or row['Motor vehicle ACTIVITY IND'] == 'Yes':\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'   \n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_sar['Offroad/Motor Vehicle/ATV ACTIVITY IND'] = df_sar.apply(lambda row: get_motor(row), axis=1)\n",
        "\n",
        "df_sar = df_sar.drop(columns=['Off road vehicle/ATV ACTIVITY IND', 'Motor vehicle ACTIVITY IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove Fugitive/Criminal Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter rows where 'Fugitive' column is 'Yes'\n",
        "fugitive_rows = df_sar[df_sar['Fugitive/Criminal IND'] == 'Yes']\n",
        "\n",
        "# Create a new dataframe with the filtered rows\n",
        "df_fugitive = fugitive_rows.copy()\n",
        "\n",
        "# Drop the filtered rows from the original dataframe\n",
        "df_sar = df_sar[df_sar['Fugitive/Criminal IND'] != 'Yes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar = df_sar.drop(columns=['Fugitive/Criminal IND'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ny.to_csv('df_ny.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sar.to_csv('df_sar.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
